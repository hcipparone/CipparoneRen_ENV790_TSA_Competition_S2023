---
title: "TSA: Forecasting Competition Instructions"
author: "Caroline Ren and Hugh Cipparone"
output: pdf_document
always_allow_html: true
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

## CREATE A REPOSITORY IN YOUR GITHUB ACCOUNT

1. Go to your user account on GitHub and navigate to the repositories tab. 

3. In the upper right corner, click the green "New" button. 

4. Name your repository with recommended naming conventions (suggestion: *Lastname1Lastname2_ENV790_TSA_Competition_S2023*). Write a short description of the purpose of the repository. Check the box to initialize the repository with a README. Add a .gitignore for R and add a GNU General Public License v3.0.

5. Invite other group members as collaborators to the repository.

## LINK YOUR REPO TO YOUR LOCAL DRIVE WITH RSTUDIO
 
1. Click the "Clone or download" button for your repository and then the "copy" icon. Make sure the box header lists "Clone with HTTPS" rather than "Clone with SSH." If not, click the "Use HTTPS" button and then copy the link.

2. Launch RStudio and select "New Project" from the File menu. Choose "Version Control" and "Git."

3. Paste the repository URL and give your repository a name and a file path.



```{r}

library(tidyverse)
library(forecast)
library(readxl)
library(tseries)
library(stats)
library(lubridate)
library(ggplot2)
library(Kendall)
library(outliers)
library(tidyverse)

#install.packages("smooth")
library(smooth)

library(rio)

```


## IMPORT THE DATASET

In the folder `/Competition/Data` you will find three datasets one with hourly demand, one with hourly temperature and another with relative humidity from January 2005 to December 2010.
Your goal is to forecast **daily** demand for the month of January 2011 based on this historical data. You may or may not use the temperature and relative humidity in your models. The temperature and humidity measurement are from stations close to the household meter data you have.

```{r}

hourly.demand<-read_excel("./Competition copy/Data/load.xlsx")
temp<-read_excel("./Competition copy/Data/temperature.xlsx")
humidity<-read_excel("./Competition copy/Data/relative_humidity.xlsx")


```


## WRANGLE/PROCESS THE DATASET

You will need to transform hourly data into daily data. See the Rmd file from Lesson 11 for instructions on how to aggregate your dataset using pipes.

Note that I provided hourly data. You shoudl take teh **average** of the 24 hours to obtain the daily load.

```{r}
#Create 24 hour averages for load
demand<-rowMeans(hourly.demand[,3:26])

daily.demand<-cbind(hourly.demand,demand)

daily.demand.edit<-daily.demand %>% 
  select(date,demand)

#create 24 hour averages for temperature
temp.sensor.mean<-rowMeans(temp[,3:30])

daily.temp<-cbind(temp,temp.sensor.mean)

daily.temp.edit<-daily.temp %>% 
  select(date,temp.sensor.mean,hr) %>% 
  pivot_wider(names_from = hr, values_from = temp.sensor.mean) 

temp.daily.mean<-rowMeans(daily.temp.edit[,2:25])

daily.temp.final<-cbind(daily.temp.edit,temp.daily.mean) %>% 
  select(date,temp.daily.mean)

#create 24 hour averages for humidity

humid.sensor.mean<-rowMeans(humidity[,3:30])

daily.humid<-cbind(humidity,humid.sensor.mean)

daily.humid.edit<-daily.humid %>% 
  select(date,humid.sensor.mean,hr) %>% 
  pivot_wider(names_from = hr, values_from = humid.sensor.mean) 

humid.daily.mean<-rowMeans(daily.humid.edit[,2:25])

daily.humid.final<-cbind(daily.humid.edit,humid.daily.mean) %>% 
  select(date,humid.daily.mean)

```


## CREATE A TIME SERIES OBJECT

After you process your dataset use the `msts()` function to create a time series object. You need to use `msts()` instead of `ts()` because your daily data will have more than one seasonal component.

```{r}

ts_demand_daily <- msts(daily.demand.edit$demand, 
                           seasonal.periods =c(7,365.25), 
                           start=c(2005,1,1))

ts_demand_daily_7 <- msts(daily.demand.edit$demand, 
                           seasonal.periods =c(7), 
                           start=c(2005,1,1))

ts_demand_daily %>% mstl() %>%
  autoplot()

```

## FIT MODELS TO YOUR DATA

Fit models to your dataset considering the period Jan 1st 2005 to Dec 31st 2009. 

```{r}

#Create training dataset from 2005 - 2009
ts_training<-window(ts_demand_daily, start=2005, end=2010)

# Model 1: Arithmetic mean on training data
Arithmetic_mean <- meanf(ts_training, h=365)

#Model 2: Seasonal naive on training data
SNAIVE_seas <- snaive(ts_training, 365)

# Model 3:  SARIMA on training data
SARIMA_autofit_training <- auto.arima(ts_training)

#Model 4: STL + ETS model on training data
ETS_fit <-  stlf(ts_training,h=365)


#Model 5: ARIMA with Fourier on training data (seasonal = False, K=c(2,12))
ARIMA_Four_fit_1 <- auto.arima(ts_training, 
                             seasonal=FALSE, 
                             lambda=0,
                             xreg=fourier(ts_training, 
                                          K=c(2,12))
                             )


#Model 6: ARIMA with Fourier on training data (seasonal = TRUE, K=c(2,12))
ARIMA_Four_fit_2 <- auto.arima(ts_training, 
                             seasonal=TRUE, 
                             lambda=0,
                             xreg=fourier(ts_training, 
                                          K=c(2,12))
                             )

#Model 7: TBATS on training data
TBATS_fit <- tbats(ts_training)


#Model 8: Neural Network with Fourier on training data
NN_fit <- nnetar(ts_training,p=1,P=0,xreg=fourier(ts_training, K=c(2,12)))


```

## FORECAST DAILY DEMAND FOR 2010 

Using the models from previous section, forecast daily demand for the period Jan 1st 2010 to Feb 28 2010. Based on the models you developed which model(s) is(are) generating good forecast? 

```{r}


# Model 1: Arithmetic mean on training data - 2010 forecast already generated

# Model 2: Seasonal naive on training data - 2010 forecast already generated

# Model 3:  SARIMA on training data
SARIMA_forecast_2010 <- forecast(object = SARIMA_autofit_training, h = 365)

# Model 4: STL + ETS - 2010 forecast already generated

# Model 5: ARIMA with Fourier on training data (seasonal = False, K=c(2,12))
ARIMA_Four_1_for <- forecast(ARIMA_Four_fit_1,
                           xreg=fourier(ts_training,
                                        K=c(2,12), #two different seasonal means models over 12 months)
                                        h=365), #forecast over next 365 days - need the xreg to capture the fourier aspect of this
                           h=365
                           ) 

#Model 6: 

ARIMA_Four_2_for <- forecast(ARIMA_Four_fit_2,
                           xreg=fourier(ts_training,
                                        K=c(2,12), #two different seasonal means models over 12 months)
                                        h=365), #forecast over next 365 days - need the xreg to capture the fourier aspect of this
                           h=365
                           ) 

#Model 7
TBATS_for <- forecast(TBATS_fit, h=365)

#Model 8
NN_for <- forecast(NN_fit, h=365,xreg=fourier(ts_training, 
                                          K=c(2,12),h=365))


#Visualizing to determine generally good models
autoplot(ts_demand_daily) +
  autolayer(Arithmetic_mean, series="Arithmetic", PI = FALSE)+
  autolayer(SNAIVE_seas, series="SEAS",PI=FALSE)+
  autolayer(SARIMA_forecast_2010, series="SARIMA 2010", PI=FALSE)+
  autolayer(ETS_fit, series="STL + ETS", PI=FALSE)+
  autolayer(ARIMA_Four_1_for, series="ARIMA_Four_1", PI=FALSE)+
  autolayer(ARIMA_Four_2_for, series="ARIMA_Four_2", PI=FALSE)+
  autolayer(TBATS_for, series="TBATS", PI=FALSE)+
  autolayer(NN_for, series="NN", PI=FALSE)+
  ylab("demand") 


#Most of these look pretty good visually - the only obviously bad ones are the SARIMA, TBATS and the Arithmetic

```

## FORECAST DAILY DEMAND FOR 2011

Just for the good model(s) you will **re-run** the model but now using the entire dataset (2005-2010) for model fitting and forecast Jan 1st 2011 to Feb 28 2011.

```{r}

#SARIMA
SARIMA_autofit_2011<-auto.arima(ts_demand_daily)
SARIMA_forecast_2011<-forecast(object = SARIMA_autofit_2011, h = 365)

#Model 2: Seasonal naive on original data
SNAIVE_seas_2011 <- snaive(ts_demand_daily, h=365)

# Model 4: STL + ETS
ETS_fit_2011 <-  stlf(ts_demand_daily,h=365)

# Model 5: ARIMA with Fourier on original data (seasonal = False, K=c(2,12))

ARIMA_Four_fit_1_2011 <- auto.arima(ts_demand_daily, 
                             seasonal=FALSE, 
                             lambda=0,
                             xreg=fourier(ts_demand_daily, 
                                          K=c(2,12))
                             )


ARIMA_Four_1_for_2011 <- forecast(ARIMA_Four_fit_1_2011,
                           xreg=fourier(ts_demand_daily,
                                        K=c(2,12), #two different seasonal means models over 12 months)
                                        h=365), #forecast over next 365 days - need the xreg to capture the fourier aspect of this
                           h=365
                           ) 

#Model 6: 

ARIMA_Four_fit_2_2011 <- auto.arima(ts_demand_daily, 
                             seasonal=TRUE, 
                             lambda=0,
                             xreg=fourier(ts_demand_daily, 
                                          K=c(2,12))
                             )


ARIMA_Four_2_for_2011 <- forecast(ARIMA_Four_fit_2_2011,
                           xreg=fourier(ts_demand_daily,
                                        K=c(2,12), #two different seasonal means models over 12 months)
                                        h=365), #forecast over next 365 days - need the xreg to capture the fourier aspect of this
                           h=365
                           ) 

#Model 8
NN_fit_2011 <- nnetar(ts_demand_daily,p=1,P=0,xreg=fourier(ts_demand_daily, K=c(2,12)))


NN_for_2011 <- forecast(NN_fit_2011, h=365,xreg=fourier(ts_demand_daily, 
                                          K=c(2,12),h=365))




##############################################################

#Test models below:

#NN with only a 7 day seasonal period time series instead of 365
NN_fit_2011_7 <- nnetar(ts_demand_daily_7,p=1,P=0,xreg=fourier(ts_demand_daily_7, K=3))


NN_for_2011_7 <- forecast(NN_fit_2011_7, h=365,xreg=fourier(ts_demand_daily_7, 
                                          K=3,h=365))


#SNAIVE with only a 7 day seasonal period time series instead of 365
SNAIVE_seas_2011_7 <- snaive(ts_demand_daily_7, h=365)


#Average of two best model outputs - SNAIVE and Neural Network - see in next chunk

#TBATS

TBATS_fit_2011 <- tbats(ts_demand_daily)

TBATS_for_2011 <- forecast(TBATS_fit_2011, h=365)


#Arithmetic

Arithmetic_mean_2011 <- meanf(ts_demand_daily, h=365)

```

## CREATE AN EXCEL FILE WITH FORECAST

Look at the excel file in your Output folder name "submission_template.csv". You will need to create your own output file with forecast for January 2011. Your file needs to be in the format of the submission template. If your forecast is a probability distribution function, consider the mean to be the point forecast.

```{r}

#create dates for the date column
date<-seq(ymd("2011-01-01"),ymd("2011-12-31"), by="days")

#select out the mean from the forecast

#SARIMA
sarima.forecast.mean<-SARIMA_forecast_2011$mean

#SNAIVE
snaive.forecast.mean<-SNAIVE_seas_2011$mean

#STL + ETS
stl.ets.forecast.mean<-ETS_fit_2011$mean

#Arima Fourier 1
arima.1.forecast.mean<-ARIMA_Four_1_for_2011$mean

#ARIMA Fourier 2
arima.2.forecast.mean<-ARIMA_Four_2_for_2011$mean

#NN
nn.forecast.mean<-NN_for_2011$mean

#NN with 7
nn.7.forecast.mean<-NN_for_2011_7$mean

#SNAIVE with 7
snaive.7.forecast.mean<-SNAIVE_seas_2011_7$mean

#Average of NN and SNAIVE (standard)
nn.snaive.forecast.mean<-(snaive.forecast.mean + nn.forecast.mean)/2

#TBATS
tbats.forecast.mean<-TBATS_for_2011$mean

#Arithmetic
arithmetic.forecast.mean<-Arithmetic_mean_2011$mean


#create the CSVs in the same format

#SARIMA
sarima.final.forecast<-data.frame(date,sarima.forecast.mean) %>% 
  slice(1:59) %>% 
  rename(load=sarima.forecast.mean) %>% 
  select(date,load)

#SNAIVE
snaive.final.forecast<-data.frame(date,snaive.forecast.mean) %>% 
  slice(1:59) %>% 
  rename(load=snaive.forecast.mean) %>% 
  select(date,load)

#STL + ETS
stl.ets.final.forecast<-data.frame(date,stl.ets.forecast.mean) %>% 
  slice(1:59) %>% 
  rename(load=stl.ets.forecast.mean) %>% 
  select(date,load)

#ARIMA Fourier 1
arima.1.final.forecast<-data.frame(date,arima.1.forecast.mean) %>% 
  slice(1:59) %>% 
  rename(load=arima.1.forecast.mean) %>% 
  select(date,load)

#ARIMA Fourier 2
arima.2.final.forecast<-data.frame(date, arima.2.forecast.mean) %>% 
  slice(1:59) %>% 
  rename(load=arima.2.forecast.mean) %>% 
  select(date,load)

#NN
nn.final.forecast<-data.frame(date, nn.forecast.mean) %>% 
  slice(1:59) %>% 
  rename(load=nn.forecast.mean) %>% 
  select(date,load)

#NN with 7
nn.7.final.forecast<-data.frame(date, nn.7.forecast.mean) %>% 
  slice(1:59) %>% 
  rename(load=nn.7.forecast.mean) %>% 
  select(date,load)

#SNAIVE with 7
snaive.7.final.forecast<-data.frame(date,snaive.7.forecast.mean) %>% 
  slice(1:59) %>% 
  rename(load=snaive.7.forecast.mean) %>% 
  select(date,load)

#NN and SNAIVE Average
nn.snaive.final.forecast<-data.frame(date,nn.snaive.forecast.mean) %>% 
  slice(1:59) %>% 
  rename(load=nn.snaive.forecast.mean) %>% 
  select(date,load)

#TBATS
tbats.final.forecast<-data.frame(date,tbats.forecast.mean) %>% 
  slice(1:59) %>% 
  rename(load=tbats.forecast.mean) %>% 
  select(date,load)

#Arithmetic
arithmetic.final.forecast<-data.frame(date,arithmetic.forecast.mean) %>% 
  slice(1:59) %>% 
  rename(load=arithmetic.forecast.mean) %>% 
  select(date,load)



#export the CSVs
write.csv(sarima.final.forecast, file="./Competition copy/Output/submission_1.csv", row.names = FALSE)

write.csv(snaive.final.forecast, file="./Competition copy/Output/submission_2.csv", row.names = FALSE)

write.csv(stl.ets.final.forecast, file="./Competition copy/Output/submission_3.csv", row.names = FALSE)

write.csv(arima.1.final.forecast, file="./Competition copy/Output/submission_4.csv", row.names = FALSE)

write.csv(arima.2.final.forecast, file="./Competition copy/Output/submission_5.csv", row.names = FALSE)

write.csv(nn.final.forecast, file="./Competition copy/Output/submission_5.csv", row.names = FALSE)

write.csv(nn.7.final.forecast, file="./Competition copy/Output/submission_6.csv", row.names = FALSE)

write.csv(snaive.7.final.forecast, file="./Competition copy/Output/submission_7.csv", row.names = FALSE)

write.csv(nn.snaive.final.forecast, file="./Competition copy/Output/submission_8.csv", row.names = FALSE)

write.csv(tbats.final.forecast, file="./Competition copy/Output/submission_9.csv", row.names = FALSE)

write.csv(arithmetic.final.forecast, file="./Competition copy/Output/submission_10.csv", row.names = FALSE)


```

## LOAD TEMPLATE IN KAGGLE

I created a kaggle competition for this assignment. You will need to enter the competition using this [invitation](https://www.kaggle.com/t/ae2a216e3daf4e91ae535183c4005e8b). 

Once you enter the competition you should be to visualize and submit your group's solution using this [link.][
https://www.kaggle.com/competitions/tsa-s23-competition/]


## COMPLETE YOUR PROJECT REPORT

For the project report you only need to organize your current Rmd file. Make sure you follow the guidelines and you provide a link to you Github repository.

1. Write in scientific style, not narrative style

2. [Global options for R chunks](https://rmarkdown.rstudio.com/lesson-3.html) should be set so that only relevant output is displayed. Turn on/off messages and warnings when applicable to avoid unnecessary outputs on the pdf.

3. Make sure your final knitted PDF looks professional. Format tables, size figures, chapters, etc.

4. Make sure the PDF file has the file name "Lastname1Lastname2_ENV790_A09_Competition.pdf" and submit it to Sakai under A09. You will only submit your PDF file.


## GRADING RUBRIC

You will be graded based on how much time and effort you put into the competition and your ability to fit a model to the data set. More specifically I will look into:

1. number of commitments to Github repo, this item will show how the team interacted and how much you worked on the project;

2. number of submissions to Kaggle platform, this will show how many models you tried and it's also an indication of how much effort the team put into the project;

3. ability to beat the vanilla/benchmark model, this will show your forecasting skills. 

The team that is leading the board when the competition ends will get extra points, but they still need to get good scores on 1 and 2. 
